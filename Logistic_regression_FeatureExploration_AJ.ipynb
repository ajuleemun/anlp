{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores feature engineering for text classification.  Your task is to create two new feature functions (like `dictionary_feature` and `unigram_feature` below), and include them in the `build_features` function.  A check grade will be given to generic features that apply across arbitrary text classification problems (e.g., a feature for bigrams); check+ will be given for at least one feature that reveals your own understanding of your data. What features do you think will help for your particular problem? Your grade is *not* tied to whether accuracy goes up or down, so be creative!  You are free to read in any other external resources you like (dictionaries, document metadata, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q0: Briefly describe your data (including the categories you're predicting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we will be using is a fake news/true news dataset curated from <a href = 'https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset'>Kaggle</a>. The dependent variable is the veracity of a news story (either True or False), with the text of the news story as the source of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import operator\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: g2p_en in /Applications/anaconda3/envs/anlp/lib/python3.8/site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.1 in /Applications/anaconda3/envs/anlp/lib/python3.8/site-packages (from g2p_en) (1.20.3)\n",
      "Requirement already satisfied: inflect>=0.3.1 in /Applications/anaconda3/envs/anlp/lib/python3.8/site-packages (from g2p_en) (5.3.0)\n",
      "Requirement already satisfied: nltk>=3.2.4 in /Applications/anaconda3/envs/anlp/lib/python3.8/site-packages (from g2p_en) (3.6.2)\n",
      "Requirement already satisfied: distance>=0.1.3 in /Applications/anaconda3/envs/anlp/lib/python3.8/site-packages (from g2p_en) (0.1.3)\n",
      "Requirement already satisfied: click in /Applications/anaconda3/envs/anlp/lib/python3.8/site-packages (from nltk>=3.2.4->g2p_en) (8.0.1)\n",
      "Requirement already satisfied: joblib in /Applications/anaconda3/envs/anlp/lib/python3.8/site-packages (from nltk>=3.2.4->g2p_en) (1.0.1)\n",
      "Requirement already satisfied: regex in /Applications/anaconda3/envs/anlp/lib/python3.8/site-packages (from nltk>=3.2.4->g2p_en) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/envs/anlp/lib/python3.8/site-packages (from nltk>=3.2.4->g2p_en) (4.62.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install g2p_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2p_en import G2p\n",
    "g2p = G2p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            label=cols[0]\n",
    "            text=cols[1]\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "directory=\"../6.classification/fake news dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY=read_data(\"%s/train.tsv\" % directory)\n",
    "devX, devY=read_data(\"%s/dev.tsv\" % directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_class(trainY, devY):\n",
    "    labelCounts=Counter()\n",
    "    for label in trainY:\n",
    "        labelCounts[label]+=1\n",
    "    majority=labelCounts.most_common(1)[0][0]\n",
    "    \n",
    "    correct=0.\n",
    "    for label in devY:\n",
    "        if label == majority:\n",
    "            correct+=1\n",
    "            \n",
    "    print(\"%s\\t%.3f\" % (majority, correct/len(devY)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll create two feature classes -- one feature class noting the presence of a word in an external dictionary, and one feature class for the word identity (i.e., unigram).  We'll implement each feature class as a function that takes a single document as input (as a list of tokens) and returns a dict corresponding to the feature we're creating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a sample dictionary we can create by inspecting the output of the Mann-Whitney test (in 2.compare/)\n",
    "dem_dictionary=set([\"republican\",\"cut\", \"opposition\"])\n",
    "repub_dictionary=set([\"growth\",\"economy\"])\n",
    "\n",
    "def political_dictionary_feature(tokens):\n",
    "    feats={}\n",
    "    for word in tokens:\n",
    "        if word in dem_dictionary:\n",
    "            feats[\"word_in_dem_dictionary\"]=1\n",
    "        if word in repub_dictionary:\n",
    "            feats[\"word_in_repub_dictionary\"]=1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_feature(tokens):\n",
    "    feats={}\n",
    "    for word in tokens:\n",
    "        feats[\"UNIGRAM_%s\" % word]=1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Add first new feature function here.  Describe your feature and why you think it will help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature we want is fairly naïve: it uses the length of the document of the text. The hypothesis is that fake news will tend to be shorter (the means of fake and true news datasets were slightly different) for reasons such as true news quoting a lot of relevant sources (and the fake news documents not). We wonder if the logistic regression may be able to use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_feature_class_one(tokens):\n",
    "    feats={}\n",
    "    feats[\"length\"]=len(tokens)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Add second new feature function here. Describe your feature and why you think it will help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second feature we use is text complexity, as measured by the Flesch-Kincaid grade level. The hypothesis is that fake news documents tend to be simpler in their language and not using as many multisyllabic words. We hope that the logistic regression is able to use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "arpabet = nltk.corpus.cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pronunciation(word):\n",
    "    if word in arpabet:\n",
    "        # pick the first pronunciation\n",
    "        return arpabet[word][0]\n",
    "\n",
    "    else:\n",
    "        return g2p(word)\n",
    "\n",
    "def get_syllable_count(word):\n",
    "    pronunciation=get_pronunciation(word)\n",
    "    sylls=0\n",
    "    for phon in pronunciation:\n",
    "        # vowels in arpabet end in digits (indicating stress)\n",
    "        if re.search(\"\\d$\", phon) is not None:\n",
    "            sylls+=1\n",
    "    return sylls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flesch_kincaid_grade_level(tokens):\n",
    "    num_words=0\n",
    "    num_sents=0\n",
    "    num_syllables=0\n",
    "    \n",
    "    #for sent in nltk.sent_tokenize(text):\n",
    "     #   sent_tokens=nltk.word_tokenize(sent)\n",
    "\n",
    "    valid_words_in_sent=False\n",
    "    for token in tokens:\n",
    "        syllables=get_syllable_count(token)\n",
    "        if syllables > 0:\n",
    "            num_syllables+=syllables\n",
    "            num_words+=1\n",
    "            # flag to ensure sentence has at least one word (as distinct from e.g. all punctuation)\n",
    "            valid_words_in_sent=True\n",
    "\n",
    "    if valid_words_in_sent:\n",
    "        num_sents+=1\n",
    "\n",
    "    fk=0.39 * (num_words/num_sents) + 11.8 * (num_syllables/num_words) - 15.59\n",
    "    return fk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_feature_class_two(tokens):\n",
    "    feats={}\n",
    "    feats[\"text complexity\"]=flesch_kincaid_grade_level(tokens)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function we'll use to aggregate together all of the information from different feature classes.  Each document has a feature dict (`feats`), and we'll update that dict with the new dict that each separate feature class is returning.  (Here you want to make sure that the keys each feature function is creating are unique so they don't get clobbered by other functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "We remove the indicators of veracity: namely, '(Reuters)', the news post location (e.g 'WASHINGTON') and other pre-text metadata in our documents that verify the information. This is an additional step of pre-processing that we do to make the fake and news datasets on as equal footing as possible, since for example, mentioning '(Reuters)' is a excellent sign of verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(trainX, feature_functions):\n",
    "    data=[]\n",
    "    for doc in trainX:\n",
    "        feats={}\n",
    "        doc = doc.lower()\n",
    "        tokens=nltk.word_tokenize(doc)\n",
    "        tokens=doc.split(\" \")\n",
    "        stopwords = ['(reuters)', '-', 'washington', 'london']\n",
    "        tokens = [i for i in tokens if i not in stopwords]\n",
    "        \n",
    "        for function in feature_functions:\n",
    "            feats.update(function(tokens))\n",
    "\n",
    "        data.append(feats)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts a dictionary of feature names to unique numerical ids\n",
    "def create_vocab(data):\n",
    "    feature_vocab={}\n",
    "    idx=0\n",
    "    for doc in data:\n",
    "        for feat in doc:\n",
    "            if feat not in feature_vocab:\n",
    "                feature_vocab[feat]=idx\n",
    "                idx+=1\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts a dictionary of feature names to a sparse representation\n",
    "# that we can fit in a scikit-learn model.  This is important because almost all feature \n",
    "# values will be 0 for most documents (note: why?), and we don't want to save them all in \n",
    "# memory.\n",
    "\n",
    "def features_to_ids(data, feature_vocab):\n",
    "    new_data=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "    for idx,doc in enumerate(data):\n",
    "        for f in doc:\n",
    "            if f in feature_vocab:\n",
    "                new_data[idx,feature_vocab[f]]=doc[f]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluates a list of feature functions on the training/dev data arguments\n",
    "def pipeline(trainX, devX, trainY, devY, feature_functions):\n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data\n",
    "    feature_vocab=create_vocab(trainX_feat)\n",
    "\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    logreg = linear_model.LogisticRegression(C=1.0, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    logreg.fit(trainX_ids, trainY)\n",
    "    print(\"Accuracy: %.3f\" % logreg.score(devX_ids, devY))\n",
    "    \n",
    "    # finding weights\n",
    "    n = 10\n",
    "    weights=logreg.coef_[0]\n",
    "    reverse_vocab=[None]*len(weights)\n",
    "    for k in feature_vocab:\n",
    "        reverse_vocab[feature_vocab[k]]=k\n",
    "    for feature, weight in sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))[:n]:\n",
    "        print(\"%.3f\\t%s\" % (weight, feature))\n",
    "    print()\n",
    "    for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
    "        print(\"%.3f\\t%s\" % (weight, feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0.518\n"
     ]
    }
   ],
   "source": [
    "majority_class(trainY,devY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the impact of different feature functions by evaluating them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.578\n",
      "0.552\tword_in_dem_dictionary\n",
      "0.939\tword_in_repub_dictionary\n",
      "\n",
      "0.939\tword_in_repub_dictionary\n",
      "0.552\tword_in_dem_dictionary\n"
     ]
    }
   ],
   "source": [
    "features=[political_dictionary_feature]\n",
    "pipeline(trainX, devX, trainY, devY, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.993\n",
      "-2.439\tUNIGRAM_more:\n",
      "-2.367\tUNIGRAM_via:\n",
      "-1.855\tUNIGRAM_obama\n",
      "-1.843\tUNIGRAM_s\n",
      "-1.647\tUNIGRAM_t\n",
      "-1.160\tUNIGRAM_image\n",
      "-1.075\tUNIGRAM_via\n",
      "-1.070\tUNIGRAM_this\n",
      "-1.065\tUNIGRAM_trump\n",
      "-1.022\tUNIGRAM_gop\n",
      "\n",
      "2.630\tUNIGRAM_on\n",
      "2.234\tUNIGRAM_said\n",
      "1.969\tUNIGRAM_said.\n",
      "1.618\tUNIGRAM_trump’s\n",
      "1.593\tUNIGRAM_u.s.\n",
      "1.387\tUNIGRAM_...\n",
      "1.273\tUNIGRAM_minister\n",
      "1.218\tUNIGRAM_“the\n",
      "1.166\tUNIGRAM_“i\n",
      "1.046\tUNIGRAM_reuters.\n"
     ]
    }
   ],
   "source": [
    "features=[political_dictionary_feature, unigram_feature]\n",
    "pipeline(trainX, devX, trainY, devY, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unigram feature is especially good at discerning between the true and fake news datasets. The reporting style of the language ('on [], [] said...') is a big contributing factor, as seen from the coefficient weights. This conforms with peer-reviewed literature that found unigrams to be effective: </br>\n",
    "<li>Ahmed H, Traore I, Saad S. “Detecting opinion spams and fake news using text classification”, Journal of Security and Privacy, Volume 1, Issue 1, Wiley, January/February 2018.</li>\n",
    "<li>Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.556\n",
      "-0.001\tlength\n",
      "\n",
      "-0.001\tlength\n"
     ]
    }
   ],
   "source": [
    "features=[new_feature_class_one]\n",
    "pipeline(trainX, devX, trainY, devY, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our length feature is certainly not as effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.555\n",
      "-0.001\ttext complexity\n",
      "\n",
      "-0.001\ttext complexity\n"
     ]
    }
   ],
   "source": [
    "features=[new_feature_class_two]\n",
    "pipeline(trainX, devX, trainY, devY, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the text complexity measure either was not as effective as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[new_feature_class_one, new_feature_class_two]\n",
    "pipeline(trainX, devX, trainY, devY, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[unigram_feature, new_feature_class_one, new_feature_class_two]\n",
    "pipeline(trainX, devX, trainY, devY, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional feature exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_feature_class_three(tokens):\n",
    "    feats={}\n",
    "    if 'http' in tokens or 'https' in tokens:\n",
    "        feats[\"hyperlink\"] = 1\n",
    "    else:\n",
    "        feats[\"hyperlink\"] = 0\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we expect that news stories that contain links are more truthful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.518\n",
      "-0.853\thyperlink\n",
      "\n",
      "-0.853\thyperlink\n"
     ]
    }
   ],
   "source": [
    "features=[new_feature_class_three]\n",
    "pipeline(trainX, devX, trainY, devY, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
